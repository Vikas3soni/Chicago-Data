# -*- coding: utf-8 -*-
"""Part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rzYpul76egH7NsWqVt9tssgXOLm_bo2W

Load the data set
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('seaborn')
import seaborn as sns
df1=pd.read_csv('Chicago_Crimes_2012_to_2017.csv',engine='python',error_bad_lines=False)

df1.head(2)

df1.columns

df=df1
df.shape

df.drop(['Unnamed: 0','ID','Case Number','Updated On','Latitude','Longitude', 'Location','IUCR'],axis=1,inplace=True)
df.columns

"""**Crime distribution over month**"""

df=df[(df.Date != '046XX N CUMBERLAND AVE')]
df.Date = pd.to_datetime(df.Date, format='%m/%d/%Y %I:%M:%S %p')
df.index = pd.DatetimeIndex(df.Date)
plt.figure(figsize=(9,5))
df.resample('M').size().plot(legend=False)
plt.title('Number of crimes over the years')
plt.xlabel('Years')
plt.ylabel('Number of crimes')
plt.show()

"""From above dataframe, i am converting it on day basis series to forecast day data. we can also use week, month forcasting kind of dataset.

### Dataframe creation
"""

def create_day_series(df1):
    
    
    day_df = pd.Series(df1.groupby(['Date']).size())
    # setting Date/Time as index
    day_df.index = pd.DatetimeIndex(day_df.index)
    # Resampling to daily trips
    day_df = day_df.resample('1D').apply(np.sum)
    
    return day_df

df_day = create_day_series(df)
df_day.head()

"""we can see that each row corresponds to a date with count of crime on that day

start and end of the data
"""

print (df_day.index.min())
print (df_day.index.max())

"""## Plotting the data set

We will use the sARIMA model for time series forcasting of the data.

setting the different library for the same.
"""

import matplotlib.pyplot as plt
import statsmodels
import statsmodels.api as sm
from statsmodels.tsa.stattools import coint, adfuller
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error
from math import sqrt
def initial_plots(time_series):
    plt.figure(1)
    plt.plot(time_series)
    plt.title('Original data across time')

    plt.show()
    
initial_plots(df_day)

"""## Test Stationary

We will get an idea about the overall trend and seasonality of the series. Then, we will use a statistical method to assess the trend and seasonality of the dataset. After trend and seasonality are assessed if they are present in the dataset, they will be removed from the series to transform the nonstationary dataset into stationary and the residuals are further analyzed.

### Test stationary using Dickey-Fuller
"""

def TestStationaryPlot(ts):
    rol_mean = ts.rolling(window = 12, center = False).mean()
    rol_std = ts.rolling(window = 12, center = False).std()
    
    plt.plot(ts, color = 'blue',label = 'Original Data')
    plt.plot(rol_mean, color = 'red', label = 'Rolling Mean')
    plt.plot(rol_std, color ='black', label = 'Rolling Std')
    plt.xticks(fontsize = 10,rotation=90)
    plt.yticks(fontsize = 10)
    
    plt.xlabel('Time in Years', fontsize = 25)
    plt.ylabel('Total Emissions', fontsize = 25)
    plt.legend(loc='best', fontsize = 15)
    plt.title('Rolling Mean & Standard Deviation', fontsize = 25)
    plt.show(block= True)

def TestStationaryAdfuller(ts, cutoff = 0.05):
    ts_test = adfuller(ts, autolag = 'AIC')
    ts_test_output = pd.Series(ts_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    
    for key,value in ts_test[4].items():
        ts_test_output['Critical Value (%s)'%key] = value
    print(ts_test_output)
    
    if ts_test[1] <= cutoff:
        print("Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary")
    else:
        print("Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary ")

TestStationaryPlot(df_day)

TestStationaryAdfuller(df_day)

"""Comment:- 


From above mean (red) and the variation in standard deviation (black line) clearly vary with time. This shows that the series has a trend. So, it is not a stationary.

### Eliminating trend and seasonality using >> Differencing

First order differencing can be done
"""

df_first = df_day - df_day.shift(1)  
TestStationaryPlot(df_first.dropna(inplace=False))

TestStationaryAdfuller(df_first.dropna(inplace=False))
df_first=df_first.dropna(inplace=False)

"""** Comment**

 First difference improves the stationarity of the series. we could able to convert the series in stationary format.

### Checking Decomposing >> Eliminating trend and seasonality
"""

from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(df_first)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(df_day, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.tight_layout()

"""### ACF and PACF plot and finding the optimal parameters

Tuning parameters (p and q) of the model by looking at the autocorrelation and partial autocorrelation graphs.
"""

plt.figure(2)
plot_acf(df_first, lags = 40)
plt.title('Autocorrelation plot')
plot_pacf(df_first, lags = 40)
plt.title('Partial autocorrelation plot')

plt.show()

# pending this partfitting model

"""### SARIMA Model (p, d, q)(P, D, Q, S):

I have take value of parameter  (p, d, q)(P, D, Q, S) as (1,2,1)(0,1,1,12) that can further be tuned.
"""

mod = sm.tsa.statespace.SARIMAX(df_day, 
                                order=(1,2,1), 
                                seasonal_order=(0,1,1,12),   
                                enforce_stationarity=False,
                                enforce_invertibility=False)
results = mod.fit()
print(results.summary())

"""The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior"""

results.plot_diagnostics(figsize=(12, 12))
plt.show()

"""Residuals of model should be  uncorrelated and normally distributed with zero-mean"""

pred = results.get_prediction(start = 1750, end = 1850, dynamic=False)
pred_ci = pred.conf_int()
pred_ci.head()

ax = df_day['2014':].plot(label='observed')
pred.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=.7)

ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='r', alpha=.5)

ax.set_xlabel('Time (years)')
ax.set_ylabel('Crime in Chicago')
plt.title('Validating predicted value', fontsize = 25)
plt.legend()
plt.show()

df_forecast = pred.predicted_mean
df_orginal = df_day['2016-10-16':]

# Compute the mean square error
mse = ((df_forecast - df_orginal) ** 2).mean()
print('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))
print('The Root Mean Square Error (RMSE) of the forcast: {:.4f}'
      .format(np.sqrt(sum((df_forecast-df_orginal)**2)/len(df_forecast))))

"""Tuning of ARIMA parameter  (p, d, q)(P, D, Q, S) will get the better results.

Grid serch approch can be taken to select/tune the parameter with AIC score, 

As that part take around 2-3 hours to tune I have left that part due to my system issue and currently Iam out of station due to some office work so have  time constraints.

## Thanks
"""